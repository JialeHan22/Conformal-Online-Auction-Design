{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53fc9faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 16:53:13.084907: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import conditionalconformal\n",
    "from conditionalconformal import CondConf\n",
    "import itertools\n",
    "import math\n",
    "from numpy import sin, cos, exp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import Dropout, LeakyReLU\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3a6f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "dimension_z = 100\n",
    "dimension_x = 100\n",
    "number_z = 30\n",
    "# Generating 30 vectors, each with 100 dimensions\n",
    "z_domain = np.random.normal(size=(number_z, dimension_z))\n",
    "\n",
    "# Generating beta_1 and beta_2\n",
    "beta_1 = np.random.uniform(-1, 1, dimension_x)\n",
    "beta_2 = np.random.uniform(-1, 1, dimension_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d394aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_function(beta_1, beta_2):\n",
    "\n",
    "    def function_2(X, Z):\n",
    "        # Calculate the epsilon (truncated normal distribution between -1 and 1)\n",
    "        epsilon = np.random.uniform(-1, 1)\n",
    "\n",
    "        # Calculate T\n",
    "        #T = np.exp(2 * (np.sin(np.dot(beta_1.T, X)) + 3) + np.cos(np.dot(beta_2.T, Z))**2) + 1 + epsilon\n",
    "        #T = np.exp(0.2*(np.dot(beta_1.T, X)) + np.cos(np.dot(beta_2.T, Z))) + 1 + epsilon\n",
    "        T = exp(abs(np.dot(beta_1.T, X))/40)*(np.cos(np.dot(beta_2.T, Z))**2)  +1+ epsilon\n",
    "        return T\n",
    "\n",
    "\n",
    "    return function_2\n",
    "\n",
    "function_2 = generate_function(beta_1, beta_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb129b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_fn(feature):\n",
    "    scalar_values = np.array(feature[:,dimension_x:(dimension_x+dimension_z)])#len 500\n",
    "\n",
    "    # Initialize the indicator matrix\n",
    "    matrix = np.zeros((len(np.array(feature[:,1])), number_z))\n",
    "\n",
    "    # Fill in the indicator matrix\n",
    "    for i, value in enumerate(scalar_values):\n",
    "        for j in range(0, number_z):\n",
    "            if  np.array_equal(value, z_domain[j]):\n",
    "                matrix[i, j] = 1\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f9fa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine X and Z into a single flattened array\n",
    "flatten_features = lambda df: np.array([np.concatenate([x, z]) for x, z in zip(df['X'], df['Z'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8806502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "Z_new = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "var_x = 8\n",
    "def conformal_predict_z_0(alpha=0.1, m=100, Z_new=Z_new, N=1000, t=200):\n",
    "    IR = []\n",
    "    for j in range(t):\n",
    "        data_points = []\n",
    "        for _ in range(N):\n",
    "            # Randomly select a vector from z_domain to be Z\n",
    "            Z = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "\n",
    "            # Compute the mean of the square of Z\n",
    "            mean_x = np.mean(Z**2)\n",
    "\n",
    "            # Generate a 150-dimensional vector X from a normal distribution with mean mean_x and variance 1\n",
    "            X = np.random.normal(mean_x, var_x, dimension_x)\n",
    "\n",
    "            # Compute T_1 and T_2 using the generated functions\n",
    "            T_1 = function_2(X, Z)\n",
    "            #T_2 = function_2(X, Z)\n",
    "\n",
    "            # Append the data point (X, Z, T_1, T_2) to the list\n",
    "            data_points.append([X, Z, T_1])\n",
    "        # Split the data into D_cali and D_train\n",
    "        D_cali = pd.DataFrame(data_points[:N//2], columns=[\"X\", \"Z\", \"T_1\"])\n",
    "        D_train = pd.DataFrame(data_points[N//2:], columns=[\"X\", \"Z\", \"T_1\"])\n",
    "\n",
    "        # Preparing data for regression\n",
    "        X_train = flatten_features(D_train)\n",
    "        # Extracting T_1 for D_train and D_cali\n",
    "        Y_train = D_train['T_1'].values\n",
    "        #构建神经网络模型\n",
    "        model_nn = Sequential([\n",
    "            InputLayer(input_shape=(X_train.shape[1],)),   \n",
    "            Dense(128, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.02)), # 使用LeakyReLU和L2正则化\n",
    "            Dropout(0.3),\n",
    "            Dense(64, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.02)),\n",
    "            Dense(1)               \n",
    "        ])\n",
    "\n",
    "        # 编译模型\n",
    "        model_nn.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "        # 在 D_train 上训练模型\n",
    "        history = model_nn.fit(X_train, Y_train, epochs=15, batch_size=32, verbose=0)\n",
    "        \n",
    "        score_fn = lambda feature,  y : abs(y - model_nn.predict(feature).flatten())\n",
    "        condCovProgram = CondConf(score_fn, phi_fn, {})\n",
    "        condCovProgram.setup_problem(flatten_features(D_cali),D_cali['T_1'].to_numpy())\n",
    "        \n",
    "        data_points_new = []\n",
    "        for _ in range(m):\n",
    "            # Compute the mean of the square of Z\n",
    "            mean_x_new = np.mean(Z_new**2)\n",
    "            # Generate a 150-dimensional vector X from a normal distribution with mean mean_x and variance 1\n",
    "            X_new = np.random.normal(mean_x_new, var_x, dimension_x)\n",
    "            # Compute T_1 and T_2 using the generated functions\n",
    "            T_1_new = function_2(X_new, Z_new)\n",
    "            #T_2_new = function_2(X_new, Z_new)\n",
    "\n",
    "            # Append the data point (X, Z, T_1, T_2) to the list\n",
    "            data_points_new.append([X_new, Z_new, T_1_new])\n",
    "\n",
    "        # Split the data into D_cali and D_train\n",
    "        D_new = pd.DataFrame(data_points_new, columns=[\"X\", \"Z\", \"T_1\"])\n",
    "        \n",
    "        X_neww = flatten_features(D_new)\n",
    "        T_1_new= D_new['T_1']\n",
    "        T_1_pred_new = model_nn.predict(X_neww).flatten()\n",
    "\n",
    "        Xtest = np.array([flatten_features(D_new)[10,:]])\n",
    "        d = condCovProgram.predict(1-alpha, Xtest, lambda x, y : x)\n",
    "        \n",
    "        # Calculate intervals\n",
    "        hat_t_L = T_1_pred_new - d\n",
    "        hat_t_U = T_1_pred_new + d \n",
    "        IR.append(np.mean((hat_t_L <= T_1_new) & (T_1_new <= hat_t_U)))\n",
    "        \n",
    "\n",
    "            \n",
    "    return IR, d\n",
    "#, hat_t_L, T_1_new, T_1_pred_new,  hat_t_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30e1a79e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "Z_new = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "\n",
    "IR, d = conformal_predict_z_0(alpha=0.1, m=50, Z_new=Z_new, N=10000, t=200)\n",
    "\n",
    "###\n",
    "plt.boxplot(IR, patch_artist=True, medianprops=dict(color=\"black\"), boxprops=dict(facecolor='lightblue'))\n",
    "plt.yticks( fontsize=10)\n",
    "plt.xticks([1], [''])  # Setting x-axis label to an empty string\n",
    "plt.axhline(y=0.9, color='red', linestyle='-', label='', linewidth=1)\n",
    "plt.xlabel('Random z0', fontsize=15)\n",
    "plt.ylabel('Coverage', fontsize=15)\n",
    "plt.yticks( fontsize=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7556ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "Z_new = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "\n",
    "\n",
    "def conformal_predict(alpha=0.1, m=100, Z_new=Z_new, N=1000):\n",
    "    data_points = []\n",
    "    for _ in range(N):\n",
    "        # Randomly select a vector from z_domain to be Z\n",
    "        Z = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "\n",
    "        # Compute the mean of the square of Z\n",
    "        mean_x = np.mean(Z**2)\n",
    "\n",
    "        # Generate a 150-dimensional vector X from a normal distribution with mean mean_x and variance 1\n",
    "        X = np.random.normal(mean_x, var_x, dimension_x)\n",
    "\n",
    "        # Compute T_1 and T_2 using the generated functions\n",
    "        T_1 = function_2(X, Z)\n",
    "        #T_2 = function_2(X, Z)\n",
    "\n",
    "        # Append the data point (X, Z, T_1, T_2) to the list\n",
    "        data_points.append([X, Z, T_1])\n",
    "    # Split the data into D_cali and D_train\n",
    "    D_cali = pd.DataFrame(data_points[:N//2], columns=[\"X\", \"Z\", \"T_1\"])\n",
    "    D_train = pd.DataFrame(data_points[N//2:], columns=[\"X\", \"Z\", \"T_1\"])\n",
    "\n",
    "    # Preparing data for regression\n",
    "    X_train = flatten_features(D_train)\n",
    "    # Extracting T_1 for D_train and D_cali\n",
    "    Y_train = D_train['T_1'].values\n",
    "    #构建神经网络模型\n",
    "    model_nn = Sequential([\n",
    "        InputLayer(input_shape=(X_train.shape[1],)),   \n",
    "        Dense(128, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.02)), # 使用LeakyReLU和L2正则化\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.02)),\n",
    "        Dense(1)                       # 输出层\n",
    "    ])\n",
    "\n",
    "    # 编译模型\n",
    "    model_nn.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # 在 D_train 上训练模型\n",
    "    history = model_nn.fit(X_train, Y_train, epochs=15, batch_size=32, verbose=0)\n",
    "    \n",
    "    \n",
    "    score_fn = lambda feature,  y : abs(y - model_nn.predict(feature).flatten())\n",
    "    condCovProgram = CondConf(score_fn, phi_fn, {})\n",
    "    condCovProgram.setup_problem(flatten_features(D_cali),D_cali['T_1'].to_numpy())\n",
    "\n",
    "    data_points_new = []\n",
    "    for _ in range(m):\n",
    "        # Compute the mean of the square of Z\n",
    "        mean_x_new = np.mean(Z_new**2)\n",
    "        # Generate a 150-dimensional vector X from a normal distribution with mean mean_x and variance 1\n",
    "        X_new = np.random.normal(mean_x_new, var_x, dimension_x)\n",
    "        # Compute T_1 and T_2 using the generated functions\n",
    "        T_1_new = function_2(X_new, Z_new)\n",
    "        #T_2_new = function_2(X_new, Z_new)\n",
    "\n",
    "        # Append the data point (X, Z, T_1, T_2) to the list\n",
    "        data_points_new.append([X_new, Z_new, T_1_new])\n",
    "\n",
    "    # Split the data into D_cali and D_train\n",
    "    D_new = pd.DataFrame(data_points_new, columns=[\"X\", \"Z\", \"T_1\"])\n",
    "\n",
    "    X_neww = flatten_features(D_new)\n",
    "    T_1_new= D_new['T_1']\n",
    "    T_1_pred_new = model_nn.predict(X_neww).flatten()\n",
    "\n",
    "    Xtest = np.array([flatten_features(D_new)[10,:]])\n",
    "    d = condCovProgram.predict(1-alpha, Xtest, lambda x, y : x)\n",
    "     \n",
    "    reserve_price = T_1_pred_new - d\n",
    "    virtual_value = np.array(T_1_new) * (np.array(T_1_new) >= reserve_price)\n",
    "\n",
    "    # Find the index(es) of the maximum virtual value\n",
    "    max_virtual_value_indexes = np.where(virtual_value == np.max(virtual_value))[0]\n",
    "    winner = None\n",
    "    if len(max_virtual_value_indexes) > 1:\n",
    "    # Check the reserve price for the max virtual value indexes\n",
    "        max_reserve_price_indexes = np.where(reserve_price[max_virtual_value_indexes] == np.max(reserve_price[max_virtual_value_indexes]))[0]\n",
    "\n",
    "        if len(max_reserve_price_indexes) > 1:\n",
    "        # If multiple maximums, choose one at random\n",
    "            winner = np.random.choice(max_virtual_value_indexes[max_reserve_price_indexes])\n",
    "        else:\n",
    "        # If only one maximum\n",
    "            winner = max_virtual_value_indexes[max_reserve_price_indexes[0]]\n",
    "    else:\n",
    "    # If only one maximum virtual value\n",
    "        winner = max_virtual_value_indexes[0]\n",
    "\n",
    "    sorted_virtual_value = np.sort(virtual_value)[::-1]\n",
    "    payment = max(0, sorted_virtual_value[1], reserve_price[winner])\n",
    "    max_welfare = np.max(np.array(T_1_new))\n",
    "    \n",
    "    return payment, payment/max_welfare, payment - np.sort(np.array(T_1_new))[::-1][1], np.sort(np.array(T_1_new))[::-1][1], max_welfare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "m_0 = 50\n",
    "IR_compare = []\n",
    "ratio = []\n",
    "Pay = []\n",
    "np.random.seed(1)\n",
    "Z_new = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "\n",
    "for i in range(200):\n",
    "    result = conformal_predict(alpha=0.1, m=m_0,  Z_new=Z_new, N = 10000)\n",
    "    IR_compare.append(result[2])\n",
    "    ratio.append(result[1])\n",
    "    Pay.append(result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "plt.xlabel('Random z0', fontsize=15)\n",
    "plt.ylabel('Revenue Difference', fontsize=15)\n",
    "plt.boxplot(IR_compare, patch_artist=True, boxprops=dict(facecolor='lightblue'),  # Box color set to blue\n",
    "            medianprops=dict(color=\"black\"))\n",
    "plt.yticks( fontsize=15)\n",
    "plt.xticks([1], [''])  # Setting x-axis label to an empty string\n",
    "plt.axhline(y=0, color='red', linestyle='-', label='', linewidth=1)\n",
    "plt.ylim(-0.75,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4dc4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 8\n",
    "def conformal_predict(alpha=0.1, m=100, Z_new=Z_new):\n",
    "\n",
    "    data_points_new = []\n",
    "    for _ in range(m):\n",
    "        # Compute the mean of the square of Z\n",
    "        mean_x_new = np.mean(Z_new**2)\n",
    "        # Generate a 150-dimensional vector X from a normal distribution with mean mean_x and variance 1\n",
    "        X_new = np.random.normal(mean_x_new, var, dimension_x)\n",
    "        # Compute T_1  using the generated functions\n",
    "        T_1_new = function_2(X_new, Z_new)\n",
    "        \n",
    "        data_points_new.append([X_new, Z_new, T_1_new])\n",
    "\n",
    "    # Split the data into D_cali and D_train\n",
    "    D_new = pd.DataFrame(data_points_new, columns=[\"X\", \"Z\", \"T_1\"])\n",
    "\n",
    "    X_neww = flatten_features(D_new)\n",
    "    T_1_new= D_new['T_1']\n",
    "    T_1_pred_new = model_nn.predict(X_neww).flatten()\n",
    "\n",
    "    Xtest = np.array([flatten_features(D_new)[10,:]])\n",
    "    d = condCovProgram.predict(1-alpha, Xtest, lambda x, y : x)\n",
    "     \n",
    "    reserve_price = T_1_pred_new - d\n",
    "    virtual_value = np.array(T_1_new) * (np.array(T_1_new) >= reserve_price)\n",
    "\n",
    "    # Find the index(es) of the maximum virtual value\n",
    "    max_virtual_value_indexes = np.where(virtual_value == np.max(virtual_value))[0]\n",
    "    winner = None\n",
    "    if len(max_virtual_value_indexes) > 1:\n",
    "    # Check the reserve price for the max virtual value indexes\n",
    "        max_reserve_price_indexes = np.where(reserve_price[max_virtual_value_indexes] == np.max(reserve_price[max_virtual_value_indexes]))[0]\n",
    "\n",
    "        if len(max_reserve_price_indexes) > 1:\n",
    "        # If multiple maximums, choose one at random\n",
    "            winner = np.random.choice(max_virtual_value_indexes[max_reserve_price_indexes])\n",
    "        else:\n",
    "        # If only one maximum\n",
    "            winner = max_virtual_value_indexes[max_reserve_price_indexes[0]]\n",
    "    else:\n",
    "    # If only one maximum virtual value\n",
    "        winner = max_virtual_value_indexes[0]\n",
    "\n",
    "    sorted_virtual_value = np.sort(virtual_value)[::-1]\n",
    "    payment = max(0, sorted_virtual_value[1], reserve_price[winner])\n",
    "    max_welfare = np.max(np.array(T_1_new))\n",
    "    \n",
    "    return payment, payment/max_welfare, payment - np.sort(np.array(T_1_new))[::-1][1], np.sort(np.array(T_1_new))[::-1][1], max_welfare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f69ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "Z_new = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "# 设置 N 的不同值\n",
    "N_values = [1000, 3000, 5000, 7000, 9000, 11000, 13000]\n",
    "num_runs = 1000\n",
    "\n",
    "# 初始化用于存储结果的字典\n",
    "results = {\"payment\": [], \"sb_value\": [], \"max_welfare\": []}\n",
    "\n",
    "# 对于每个 N 值，运行函数 200 次并计算平均值\n",
    "for N in N_values:\n",
    "    data_points = []\n",
    "    for _ in range(N):\n",
    "        # Randomly select a vector from z_domain to be Z\n",
    "        Z = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "\n",
    "        # Compute the mean of the square of Z\n",
    "        mean_x = np.mean(Z**2)\n",
    "\n",
    "        # Generate a 150-dimensional vector X from a normal distribution with mean mean_x and variance 1\n",
    "        X = np.random.normal(mean_x, var, dimension_x)\n",
    "\n",
    "        # Compute T_1 and T_2 using the generated functions\n",
    "        T_1 = function_2(X, Z)\n",
    "        #T_2 = function_2(X, Z)\n",
    "\n",
    "        # Append the data point (X, Z, T_1, T_2) to the list\n",
    "        data_points.append([X, Z, T_1])\n",
    "    # Split the data into D_cali and D_train\n",
    "    D_cali = pd.DataFrame(data_points[:N//2], columns=[\"X\", \"Z\", \"T_1\"])\n",
    "    D_train = pd.DataFrame(data_points[N//2:], columns=[\"X\", \"Z\", \"T_1\"])\n",
    "\n",
    "    # Preparing data for regression\n",
    "    X_train = flatten_features(D_train)\n",
    "    # Extracting T_1 for D_train and D_cali\n",
    "    Y_train = D_train['T_1'].values\n",
    "    #构建神经网络模型\n",
    "    model_nn = Sequential([\n",
    "        InputLayer(input_shape=(X_train.shape[1],)),   \n",
    "        Dense(128, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.02)), # 使用LeakyReLU和L2正则化\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.02)),\n",
    "        Dense(1)                          # 输出层\n",
    "    ])\n",
    "\n",
    "    # 编译模型\n",
    "    model_nn.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # 在 D_train 上训练模型\n",
    "    history = model_nn.fit(X_train, Y_train, epochs=15, batch_size=32, verbose=0)\n",
    "    score_fn = lambda feature,  y : abs(y - model_nn.predict(feature).flatten())\n",
    "    condCovProgram = CondConf(score_fn, phi_fn, {})\n",
    "    condCovProgram.setup_problem(flatten_features(D_cali),D_cali['T_1'].to_numpy())\n",
    "    \n",
    "    payments, sb_values, max_welfares = [], [], []\n",
    "    for _ in range(num_runs):\n",
    "        payment, _, _, sb_value, max_welfare = conformal_predict(0.1, 50, Z_new)\n",
    "        payments.append(payment)\n",
    "        sb_values.append(sb_value)\n",
    "        max_welfares.append(max_welfare)\n",
    "    results[\"payment\"].append(np.mean(payments))\n",
    "    results[\"sb_value\"].append(np.mean(sb_values))\n",
    "    results[\"max_welfare\"].append(np.mean(max_welfares))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "plt.figure(figsize=(5,5))  # 修改此处以创建正方形的图形\n",
    "plt.plot(N_values[0:7], results[\"max_welfare\"][0:7], label=\"Max Welfare\", linewidth=2)  # 加粗线条\n",
    "plt.plot(N_values[0:7], results[\"payment\"][0:7], label=\"Payment\", linewidth=2)  # 加粗线条\n",
    "plt.plot(N_values[0:7], results[\"sb_value\"][0:7], label=\"2nd Highest Price\", linewidth=2)  # 加粗线条\n",
    "\n",
    "\n",
    "# 设置坐标轴标签和字体大小\n",
    "plt.xlabel(\"Number of Previous Data ($N$)\", fontsize=15)\n",
    "plt.ylabel(\"Expected Values\", fontsize=15)\n",
    "\n",
    "# 设置图例和字体大小\n",
    "plt.legend(fontsize=15, bbox_to_anchor=(0.3, 0.5), loc='center left')\n",
    "\n",
    "# 设置横坐标为对数尺度，并确保刻度清晰\n",
    "#plt.xscale(\"log\")\n",
    "\n",
    "# 设置刻度标签的字体大小\n",
    "plt.xticks(N_values[0:7], labels=[str(N) for N in N_values[0:7]], fontsize=30)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6fca1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 18:21:02.896145: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 18:21:02.925721: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 71/313 [=====>........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 18:22:05.302151: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "Z_new = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "\n",
    "data_points = []\n",
    "N = 20000\n",
    "for _ in range(N):\n",
    "    # Randomly select a vector from z_domain to be Z\n",
    "    Z = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "\n",
    "    # Compute the mean of the square of Z\n",
    "    mean_x = np.mean(Z**2)\n",
    "\n",
    "    # Generate a 150-dimensional vector X from a normal distribution with mean mean_x and variance 1\n",
    "    X = np.random.normal(mean_x, var_x, dimension_x)\n",
    "\n",
    "    # Compute T_1 and T_2 using the generated functions\n",
    "    T_1 = function_2(X, Z)\n",
    "    #T_2 = function_2(X, Z)\n",
    "\n",
    "    # Append the data point (X, Z, T_1, T_2) to the list\n",
    "    data_points.append([X, Z, T_1])\n",
    "# Split the data into D_cali and D_train\n",
    "D_cali = pd.DataFrame(data_points[:N//2], columns=[\"X\", \"Z\", \"T_1\"])\n",
    "D_train = pd.DataFrame(data_points[N//2:], columns=[\"X\", \"Z\", \"T_1\"])\n",
    "\n",
    "# Preparing data for regression\n",
    "X_train = flatten_features(D_train)\n",
    "# Extracting T_1 for D_train and D_cali\n",
    "Y_train = D_train['T_1'].values\n",
    "#构建神经网络模型\n",
    "model_nn = Sequential([\n",
    "    InputLayer(input_shape=(X_train.shape[1],)),   \n",
    "    Dense(128, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.02)), # 使用LeakyReLU和L2正则化\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation=LeakyReLU(alpha=0.01), kernel_regularizer=l2(0.02)),\n",
    "    Dense(1)                       # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model_nn.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# 在 D_train 上训练模型\n",
    "history = model_nn.fit(X_train, Y_train, epochs=15, batch_size=32, verbose=0)\n",
    "\n",
    "score_fn = lambda feature,  y : abs(y - model_nn.predict(feature).flatten())\n",
    "condCovProgram = CondConf(score_fn, phi_fn, {})\n",
    "condCovProgram.setup_problem(flatten_features(D_cali),D_cali['T_1'].to_numpy())\n",
    "\n",
    "def conformal_predict(alpha=0.1, m=100, Z_new=Z_new):\n",
    "\n",
    "    data_points_new = []\n",
    "    for _ in range(m):\n",
    "        # Compute the mean of the square of Z\n",
    "        mean_x_new = np.mean(Z_new**2)\n",
    "        # Generate a 150-dimensional vector X from a normal distribution with mean mean_x and variance 1\n",
    "        X_new = np.random.normal(mean_x_new, var_x, dimension_x)\n",
    "        # Compute T_1 and T_2 using the generated functions\n",
    "        T_1_new = function_2(X_new, Z_new)\n",
    "        #T_2_new = function_2(X_new, Z_new)\n",
    "\n",
    "        # Append the data point (X, Z, T_1, T_2) to the list\n",
    "        data_points_new.append([X_new, Z_new, T_1_new])\n",
    "\n",
    "    # Split the data into D_cali and D_train\n",
    "    D_new = pd.DataFrame(data_points_new, columns=[\"X\", \"Z\", \"T_1\"])\n",
    "\n",
    "    X_neww = flatten_features(D_new)\n",
    "    T_1_new= D_new['T_1']\n",
    "    T_1_pred_new = model_nn.predict(X_neww).flatten()\n",
    "\n",
    "    Xtest = np.array([flatten_features(D_new)[10,:]])\n",
    "    d = condCovProgram.predict(1-alpha, Xtest, lambda x, y : x)\n",
    "     \n",
    "    reserve_price = T_1_pred_new - d\n",
    "    virtual_value = np.array(T_1_new) * (np.array(T_1_new) >= reserve_price)\n",
    "\n",
    "    # Find the index(es) of the maximum virtual value\n",
    "    max_virtual_value_indexes = np.where(virtual_value == np.max(virtual_value))[0]\n",
    "    winner = None\n",
    "    if len(max_virtual_value_indexes) > 1:\n",
    "    # Check the reserve price for the max virtual value indexes\n",
    "        max_reserve_price_indexes = np.where(reserve_price[max_virtual_value_indexes] == np.max(reserve_price[max_virtual_value_indexes]))[0]\n",
    "\n",
    "        if len(max_reserve_price_indexes) > 1:\n",
    "        # If multiple maximums, choose one at random\n",
    "            winner = np.random.choice(max_virtual_value_indexes[max_reserve_price_indexes])\n",
    "        else:\n",
    "        # If only one maximum\n",
    "            winner = max_virtual_value_indexes[max_reserve_price_indexes[0]]\n",
    "    else:\n",
    "    # If only one maximum virtual value\n",
    "        winner = max_virtual_value_indexes[0]\n",
    "\n",
    "    sorted_virtual_value = np.sort(virtual_value)[::-1]\n",
    "    payment = max(0, sorted_virtual_value[1], reserve_price[winner])\n",
    "    max_welfare = np.max(np.array(T_1_new))\n",
    "    \n",
    "    return payment, payment/max_welfare, payment - np.sort(np.array(T_1_new))[::-1][1], np.sort(np.array(T_1_new))[::-1][1], max_welfare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 设置 m 的不同值\n",
    "m_values = list(range(50, 301, 50))\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "Z_new = z_domain[np.random.randint(z_domain.shape[0])]\n",
    "num_runs = 500\n",
    "\n",
    "# 初始化用于存储结果的字典\n",
    "results_m = {\"payment\": [], \"y_value\": [], \"max_welfare\": []}\n",
    "\n",
    "# 对于每个 m 值，运行函数 100 次并计算平均值\n",
    "for m in m_values:\n",
    "    payments, y_values, max_welfares = [], [], []\n",
    "    for _ in range(num_runs):\n",
    "        payment, _, _, y_value, max_welfare = conformal_predict(0.1, m, Z_new)\n",
    "        payments.append(payment)\n",
    "        y_values.append(y_value)\n",
    "        max_welfares.append(max_welfare)\n",
    "    results_m[\"payment\"].append(np.mean(payments))\n",
    "    results_m[\"y_value\"].append(np.mean(y_values))\n",
    "    results_m[\"max_welfare\"].append(np.mean(max_welfares))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fdcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# 绘制结果图\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(m_values, results_m[\"max_welfare\"], label=\"Max Welfare\")\n",
    "plt.plot(m_values, results_m[\"payment\"], label=\"Payment\")\n",
    "plt.plot(m_values, results_m[\"y_value\"], label=\"2nd Highest Price\")\n",
    "\n",
    "plt.xlabel(\"Number of Bidders $(m)$\", fontsize=20)\n",
    "plt.ylabel(\"Expected Values\", fontsize=30)\n",
    "plt.yticks(fontsize=25)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.title(\"\")#Average Payment, 2nd Best Price, and Max Welfare for Different m Values\n",
    "# 设置图例和字体大小\n",
    "plt.legend(fontsize=20, loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
